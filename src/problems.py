from __future__ import annotations

import random
from abc import ABC, abstractmethod

import numpy as np
from sklearn.datasets import make_moons


class Problem(ABC):
    def __init__(self, a_matrix: np.ndarray, y: np.ndarray):
        """
        A minimization problem.

        :param a_matrix: the design matrix A (number of points, number of dimensions)
        :param y: response vector (number of points, 1)
        """
        self.a_matrix = a_matrix
        self.y = y

    @property
    def n(self):
        #  number of points
        return self.a_matrix.shape[0]

    @property
    def d(self):
        #  number of dimensions
        return self.a_matrix.shape[1]

    def a_matrix_i(self, i: int):
        """
        ith row of the A matrix
            i = 1, ..., n

        :param i: row index
        :return: row vector of shape (1, number of dimensions)
        """
        return self.a_matrix[i : i + 1, :]

    def a_matrix_j(self, j: int):
        """
        jth column of the A matrix
            j = 1, ..., d

        :param j: column index
        :return: column vector of shape (number of points, 1)
        """
        return self.a_matrix[:, j : j + 1]

    @staticmethod
    @abstractmethod
    def proximity_operator(
        x: np.ndarray, gamma_parameter: float, lambda_parameter: float
    ) -> np.ndarray:
        """
        Proximity operator attached to g(x) for the minimisation problem.

        :param x: current solution (number of dimension, 1)
        :param gamma_parameter: scalar dictating the step size
        :param lambda_parameter: scalar on g(x) of the minimization problem
        :return: prox(x) vector (number of dimensions, 1)
        """
        pass

    @abstractmethod
    def loss(self, x: np.ndarray, lambda_parameter: float) -> float:
        """
        Calculation of loss function with respect to x and lambda

        :param x: current solution (number of dimension, 1)
        :param lambda_parameter: scalar on g(x) of the minimization problem
        :return: loss function evaluation at x with lambda
        """
        pass

    @staticmethod
    @abstractmethod
    def generate(*args, **kwargs) -> Problem:
        """
        Instantiate a problem.
        """
        pass

    @abstractmethod
    def grad_f_j(self, x: np.ndarray, j: int) -> float:
        """
        Gradient of the function at the jth dimension
            j = 1, ..., d

        :param x: current solution (number of dimension, 1)
        :param j: dimension for which we want the gradient
        :return: the gradient of the jth dimension at x
        """
        pass

    @abstractmethod
    def grad_f(self, x: np.ndarray) -> np.ndarray:
        """
        Gradient of the function evaluated at x.

        :param x: current solution (number of dimension, 1)
        :return: gradient vector (number of dimension, 1)
        """
        pass


class SparseProblem(Problem):
    def __init__(
        self,
        a_matrix: np.ndarray,
        y: np.ndarray,
        x_sparse: np.ndarray,
        sparsity: int,
        std: float,
    ):
        """
        The response y is generated by the process:
            y = A@x_sparse + epsilon

        :param a_matrix: the design matrix of shape (number of points, number of dimensions)
        :param y: the response vector of shape (number of points, 1)
        :param x_sparse: a sparse vector, meaning it only uses a small number of dimensions of A when generating y
                         (number of dimensions, 1)
        :param sparsity: the number of non-zero elements in x_sparse
        :param std: standard deviation parameterizing epsilon, the noise added to the observed response y
        """
        self.x_sparse = x_sparse
        self.sparsity = sparsity
        self.std = std
        super().__init__(a_matrix, y)

    @staticmethod
    def proximity_operator(
        x: np.ndarray, gamma_parameter: float, lambda_parameter: float
    ) -> np.ndarray:
        """
        The soft thresholding operator, the proximity operator for the absolute value function:
            lambda*||Â·||

        :param x: current solution (number of dimension, 1)
        :param gamma_parameter: scalar dictating the step size
        :param lambda_parameter: scalar on g(x) of the minimization problem
        :return: soft_threshold(x) vector (number of dimensions, 1)
        """
        rho = gamma_parameter * lambda_parameter
        return np.sign(x - rho) * np.maximum(0, np.abs(x) - rho)

    def loss(self, x: np.ndarray, lambda_parameter: float) -> float:
        """

        :param x: current solution (number of dimension, 1)
        :param lambda_parameter: scalar on g(x) of the minimization problem
        :return: LASSO loss evaluation at x with lambda
        """
        return 0.5 * np.mean(
            (self.a_matrix @ x - self.y) ** 2
        ) + lambda_parameter * np.linalg.norm(x, ord=1)

    def grad_f_j(self, x: np.ndarray, j: int) -> float:
        """
        Gradient <a_j, Ax-y>
            j = 1, ..., d

        :param x: current solution (number of dimension, 1)
        :param j: dimension for which we want the gradient
        :return: the gradient of the jth dimension at x
        """
        return self.a_matrix_j(j).T @ (self.a_matrix @ x - self.y)

    def grad_f(self, x: np.ndarray) -> np.ndarray:
        """
        Gradient of the function evaluated at x.

        :param x: current solution (number of dimension, 1)
        :return: gradient vector (number of dimension, 1)
        """
        pass

    @staticmethod
    def generate(
        number_of_samples, number_of_dimensions, sparsity: int, std=0.06
    ) -> SparseProblem:
        """
        Generate xs vectors with entries in [0.5, 1] and [-1, -0.5] respectively.

        :param number_of_samples: number of samples to generate
        :param number_of_dimensions: number of dimensions to generate
        :param sparsity: the number of non-zero elements in x_sparse
        :param std: standard deviation parameterizing epsilon, the noise added to the observed response y
        :return: spare problem
        """

        assert sparsity % 2 == 0, f"{sparsity=} needs to be divisible by 2"
        xsp = 0.5 * (np.random.rand(sparsity // 2) + 1)
        xsn = -0.5 * (np.random.rand(sparsity // 2) + 1)
        x_sparse = np.hstack([xsp, xsn, np.zeros(number_of_dimensions - sparsity)])
        random.shuffle(x_sparse)

        # Generate A
        a_matrix = np.random.randn(number_of_samples, number_of_dimensions)

        # Generate eps
        y = a_matrix @ x_sparse + std * np.random.randn(number_of_samples)
        return SparseProblem(
            a_matrix=a_matrix,
            y=y.reshape(-1, 1),
            x_sparse=x_sparse.reshape(-1, 1),
            sparsity=sparsity,
            std=std,
        )


class HalfMoonsProblem(Problem):
    def __init__(self, x: np.ndarray, y: np.ndarray, sigma: float):
        """
        Classification dataset

        :param x: design_matrix of training points (number of training points, 2)
        :param y: the response label -1 or 1 (number of training points, 1)
        :param sigma: kernel length scale
        """
        self.x = x
        self.sigma = sigma
        self.gram_matrix = self.calculate_gram(sigma, x)
        super().__init__(
            a_matrix=np.multiply(y @ y.T, self.gram_matrix),
            y=y,
        )

    @staticmethod
    def kernel(x: np.ndarray, y: np.ndarray, sigma: float) -> float:
        """
        Square exponential kernel.

        :param x: data vector (2, 1)
        :param y: data vector (2, 1)
        :param sigma: kernel length scale
        :return: kernel evaluation k(x, y)
        """
        diff = x - y
        return np.exp(-diff.T @ diff / (2 * sigma**2)).item()

    @staticmethod
    def calculate_gram(sigma: float, x: np.ndarray, y: np.ndarray = None) -> np.ndarray:
        """
        Gram matrix of kernel evaluated at each training point and each test point

        :param x: design_matrix of train points (number of train points, 2)
        :param y: design_matrix of test points (number of test points, 2)
        :param sigma: kernel length scale
        :return: gram matrix (number of training points, number of test points)
        """
        if y is None:
            y = x
        n = x.shape[0]
        m = y.shape[0]
        gram_matrix = np.zeros((n, m))
        for i in range(n):
            for j in range(m):
                gram_matrix[i, j] = HalfMoonsProblem.kernel(
                    x[i : i + 1, :].T, y[j : j + 1, :].T, sigma
                )
        return gram_matrix

    def proximity_operator(
        self, x: np.ndarray, gamma_parameter: float, lambda_parameter: float
    ) -> np.ndarray:
        """
        Projection to the convex set [0, 1/(lambda*n)]^n

        :param x: current solution (number of training points, 1)
        :param gamma_parameter: unused
        :param lambda_parameter: scalar on g(x) of the minimization problem
        :return: projection(x) vector (number of dimensions, 1)
        """
        return np.clip(x, 0, 1 / (lambda_parameter * self.n))

    @staticmethod
    def indicator_function(x) -> float:
        """
        Indicator function on [0, 1]^n

        :param x: current solution (number of training points, 1)
        :return: if x lies in [0, 1]^n then 0, otherwise infinity
        """
        return float("inf") if np.sum(x < 0) + np.sum(x > 1) else 0

    def loss(self, x: np.ndarray, lambda_parameter: float) -> float:
        """
        Loss function for classification problem

        :param x: current solution (number of training points, 1)
        :param lambda_parameter: scalar on g(x) of the minimization problem
        :return: current loss
        """
        scale = 1 / (lambda_parameter * self.n)
        return (
            0.5 * x.T @ self.a_matrix @ x
            - scale * np.sum(x)
            + self.indicator_function(x / scale)
        ).item()

    def grad_f_j(self, x: np.ndarray, j) -> float:
        """
        Gradient of <alpha*, K_y alpha> w.r.t. alpha_j
            j = 1, ..., d

        :param x: current solution (number of dimension, 1)
        :param j: dimension for which we want the gradient
        :return: the gradient of the jth dimension at x
        """
        return self.a_matrix_j(j).T @ x

    def grad_f(self, x: np.ndarray) -> np.ndarray:
        """
        Gradient of f(-A^* alpha)

        :param x: current solution (number of dimension, 1)
        :return: the gradient of f evaluated at -A^* alpha
        """
        return -self.a_matrix.T @ x

    @staticmethod
    def generate(
        number_of_samples: int, noise: float, sigma: float, random_state: int
    ) -> HalfMoonsProblem:
        """
        Generate a Half Moons Probelm

        :param number_of_samples: number of samples to generate
        :param noise: noise in data
        :param sigma: kernel length scale
        :param random_state: randomisation statea
        :return: Half Moons Problem
        """

        x, y = make_moons(
            n_samples=number_of_samples, noise=noise, random_state=random_state
        )
        y = 2 * y - 1
        return HalfMoonsProblem(
            x=x,
            y=y.reshape(-1, 1),
            sigma=sigma,
        )

    def predict(self, alpha: np.ndarray, x: np.ndarray) -> np.ndarray:
        """
        Make a classification prediction

        :param alpha: current weight vector for each training point (number of dimension, 1)
        :param x: test points (number of test points, 2)
        :return: predictions vector (number of test points, 1) where each element is -1 or 1
        """
        gram_matrix = self.calculate_gram(self.sigma, self.x, x)
        return np.sign(
            gram_matrix.T @ np.multiply(self.y.reshape(-1, 1), alpha.reshape(-1, 1))
        ).reshape(-1)
